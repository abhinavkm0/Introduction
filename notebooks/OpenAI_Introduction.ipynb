{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac3d7b1",
   "metadata": {},
   "source": [
    "# OpenAI Introduction\n",
    "\n",
    "Use this for [reference](https://platform.openai.com/docs/api-reference/authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pipenv install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e56514",
   "metadata": {},
   "source": [
    "> Create an OpenAI account and get an API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f543321",
   "metadata": {},
   "source": [
    "### OpenAI:\n",
    "\n",
    "1. Chat completion API ✅\n",
    "2. Implement structured output ✅\n",
    "3. Implement tool calling ✅\n",
    "4. Implement streaming ✅\n",
    "5. Implement assistant ✅\n",
    "6. Response API ✅\n",
    "7. Chaining of response ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07b8812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82915665",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026dccaf",
   "metadata": {},
   "source": [
    "### List Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e7e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "model_names = [model.id for model in models if \"gpt\" in model.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459f0e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-35-turbo-0301',\n",
       " 'gpt-35-turbo-0613',\n",
       " 'gpt-35-turbo-1106',\n",
       " 'gpt-35-turbo-0125',\n",
       " 'gpt-35-turbo-instruct-0914',\n",
       " 'gpt-35-turbo-16k-0613',\n",
       " 'gpt-4-0125-Preview',\n",
       " 'gpt-4-1106-Preview',\n",
       " 'gpt-4-0314',\n",
       " 'gpt-4-0613',\n",
       " 'gpt-4-32k-0314',\n",
       " 'gpt-4-32k-0613',\n",
       " 'gpt-4-vision-preview',\n",
       " 'gpt-4-turbo-2024-04-09',\n",
       " 'gpt-4-turbo-jp',\n",
       " 'gpt-4o-2024-05-13',\n",
       " 'gpt-4o-2024-08-06',\n",
       " 'gpt-4o-mini-2024-07-18',\n",
       " 'gpt-4o-2024-11-20',\n",
       " 'gpt-4o-audio-mai',\n",
       " 'gpt-4o-realtime-preview',\n",
       " 'gpt-4o-mini-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-canvas-2024-09-25',\n",
       " 'gpt-4o-audio-preview-2024-10-01',\n",
       " 'gpt-4o-audio-preview-2024-12-17',\n",
       " 'gpt-4o-mini-audio-preview-2024-12-17',\n",
       " 'gpt-4o-transcribe-2025-03-20',\n",
       " 'gpt-4o-mini-transcribe-2025-03-20',\n",
       " 'gpt-4o-mini-tts-2025-03-20',\n",
       " 'gpt-4.1-2025-04-14',\n",
       " 'gpt-4.1-mini-2025-04-14',\n",
       " 'gpt-4.1-nano-2025-04-14',\n",
       " 'gpt-35-turbo',\n",
       " 'gpt-35-turbo-instruct',\n",
       " 'gpt-35-turbo-16k',\n",
       " 'gpt-4',\n",
       " 'gpt-4-32k',\n",
       " 'gpt-4o',\n",
       " 'gpt-4o-mini',\n",
       " 'gpt-4o-transcribe',\n",
       " 'gpt-4o-mini-transcribe',\n",
       " 'gpt-4o-mini-tts',\n",
       " 'gpt-4.1',\n",
       " 'gpt-4.1-mini',\n",
       " 'gpt-4.1-nano']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f99367",
   "metadata": {},
   "source": [
    "| 1. Chat Completion |\n",
    "|--|\n",
    "\n",
    "\n",
    "The Chat Completions API endpoint will generate a model response from a list of messages comprising a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be71f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c3963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=MODEL_NAME,\n",
    "  store=False,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e36c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent thoughts in code,  \n",
      "Minds of circuits weave and learn,  \n",
      "Dreams without a soul.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d1877",
   "metadata": {},
   "source": [
    "| 2. Structured Output |\n",
    "|--|\n",
    "\n",
    "Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied JSON Schema,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959601f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93491fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserRegistration(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    tier: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94404453",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the user information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"We have a new user named Tim aged 25 with a tier 3 account.\"},\n",
    "    ],\n",
    "    response_format=UserRegistration,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b37cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = completion.choices[0].message.parsed\n",
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a3708",
   "metadata": {},
   "source": [
    "`json_schem` is an alternate method for passing schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6359d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from json import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e080cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"math_response\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"steps\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"explanation\": {\"type\": \"string\"},\n",
    "                                \"output\": {\"type\": \"string\"}\n",
    "                            },\n",
    "                            \"required\": [\"explanation\", \"output\"],\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_answer\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"steps\", \"final_answer\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea7eb367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'final_answer': 'x = -3.75 or x = -15/4',\n",
      "    'steps': [   {   'explanation': 'Start with the original equation: 8x + 7 '\n",
      "                                    '= -23.',\n",
      "                     'output': '8x + 7 = -23'},\n",
      "                 {   'explanation': 'Subtract 7 from both sides to isolate the '\n",
      "                                    'term with x.',\n",
      "                     'output': '8x = -23 - 7'},\n",
      "                 {   'explanation': 'Now simplify the right side: -23 - 7 = '\n",
      "                                    '-30.',\n",
      "                     'output': '8x = -30'},\n",
      "                 {   'explanation': 'Next, divide both sides by 8 to solve for '\n",
      "                                    'x.',\n",
      "                     'output': 'x = -30 / 8'},\n",
      "                 {   'explanation': 'Now simplify the fraction: -30 / 8 = -15 '\n",
      "                                    '/ 4 or -3.75.',\n",
      "                     'output': 'x = -3.75 or x = -15/4'}]}\n"
     ]
    }
   ],
   "source": [
    "pprint(loads(response.choices[0].message.content), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fa94ab",
   "metadata": {},
   "source": [
    "Streaming with Structured Output (JSON Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9adfb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='', choices=[], created=0, model='', object='', service_tier=None, system_fingerprint=None, usage=None, prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content='{\"', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content='steps', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content='\":[', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content='{\"', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content='ex', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content='planation', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content='\":\"', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content='Start', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-Bbn2Q5h0aZ5Lqu7Azp9Bg4tdvUw50', choices=[Choice(delta=ChoiceDelta(content=' with', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1748346338, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
    "    ],\n",
    "    stream=True,\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"math_response\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"steps\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"explanation\": {\"type\": \"string\"},\n",
    "                                \"output\": {\"type\": \"string\"}\n",
    "                            },\n",
    "                            \"required\": [\"explanation\", \"output\"],\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    },\n",
    "                    \"final_answer\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"steps\", \"final_answer\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "cnt = 0\n",
    "for chunk in response:\n",
    "    print(chunk)\n",
    "    print(\"****************\")\n",
    "    \n",
    "    if cnt == 10:\n",
    "        break\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957bb6d",
   "metadata": {},
   "source": [
    "> Simply passing `stream` to `True` for the Completion api does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901738e8",
   "metadata": {},
   "source": [
    "Most if not all solutions this involves writing our own parser. I'll be trying with the `openai-streaming` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e60395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5144096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntitiesModel(BaseModel):\n",
    "    attributes: List[str]\n",
    "    colors: List[str]\n",
    "    animals: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37fc2fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content.delta parsed: {}\n",
      "content.delta parsed: {}\n",
      "content.delta parsed: {'attributes': []}\n",
      "content.delta parsed: {'attributes': []}\n",
      "content.delta parsed: {'attributes': ['quick']}\n",
      "content.delta parsed: {'attributes': ['quick']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': []}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': []}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': ['blue']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': ['blue']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': ['blue']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': ['blue'], 'animals': []}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': ['blue'], 'animals': []}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': ['blue'], 'animals': ['fox']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': ['blue'], 'animals': ['fox']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': ['blue'], 'animals': ['fox', 'dog']}\n",
      "content.delta parsed: {'attributes': ['quick', 'brown', 'lazy', 'piercing', 'blue'], 'colors': ['blue'], 'animals': ['fox', 'dog']}\n",
      "content.done\n",
      "Final completion: ParsedChatCompletion[EntitiesModel](id='chatcmpl-Bbnj11ZHIarXxl6BKybMUL2fV1ka9', choices=[ParsedChoice[EntitiesModel](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[EntitiesModel](content='{\"attributes\":[\"quick\",\"brown\",\"lazy\",\"piercing\",\"blue\"],\"colors\":[\"blue\"],\"animals\":[\"fox\",\"dog\"]}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=EntitiesModel(attributes=['quick', 'brown', 'lazy', 'piercing', 'blue'], colors=['blue'], animals=['fox', 'dog'])))], created=1748348979, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=None)\n"
     ]
    }
   ],
   "source": [
    "with client.beta.chat.completions.stream(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract entities from the input text\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The quick brown fox jumps over the lazy dog with piercing blue eyes\",\n",
    "        },\n",
    "    ],\n",
    "    response_format=EntitiesModel,\n",
    ") as stream:\n",
    "    for event in stream:\n",
    "        if event.type == \"content.delta\":\n",
    "            if event.parsed is not None:\n",
    "                # Print the parsed data as JSON\n",
    "                print(\"content.delta parsed:\", event.parsed)\n",
    "        elif event.type == \"content.done\":\n",
    "            print(\"content.done\")\n",
    "        elif event.type == \"error\":\n",
    "            print(\"Error in stream:\", event.error)\n",
    "\n",
    "final_completion = stream.get_final_completion()\n",
    "print(\"Final completion:\", final_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133937da",
   "metadata": {},
   "source": [
    "| 3. Tool/Function Calling |\n",
    "|--|\n",
    "\n",
    "Function calling provides a powerful and flexible way for OpenAI models to interface with your code or external services. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"location\"\n",
    "            ],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b6d26",
   "metadata": {},
   "source": [
    "| Function -> Tool JSON Convertor |\n",
    "|--|\n",
    "\n",
    "Automate tool JSON creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import Callable, get_type_hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_schema(func: Callable) -> str:\n",
    "    \"\"\"\n",
    "    Generate a JSON schema for a given function based on its docstring and type hints.\n",
    "\n",
    "    Args:\n",
    "        func (Callable): The Python function to generate a schema for.\n",
    "\n",
    "    Returns:\n",
    "        str: A JSON string representing the function's schema.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the function lacks a docstring.\n",
    "    \"\"\"\n",
    "    # Get function metadata\n",
    "    if not func.__doc__:\n",
    "        raise ValueError(\"Function must have a docstring\")\n",
    "\n",
    "    signature = inspect.signature(func)\n",
    "    type_hints = get_type_hints(func)\n",
    "\n",
    "    # Parse docstring\n",
    "    docstring = inspect.getdoc(func) or \"\"\n",
    "    doc_lines = docstring.split('\\n')\n",
    "    description = doc_lines[0].strip()  # First line is the main description\n",
    "    param_descriptions = {}\n",
    "    in_args_section = False\n",
    "\n",
    "    # Extract parameter descriptions\n",
    "    for line in doc_lines:\n",
    "        line = line.strip()\n",
    "        if line.lower().startswith('args:'):\n",
    "            in_args_section = True\n",
    "            continue\n",
    "        if in_args_section and line and ':' in line:\n",
    "            # Handle lines like \"param_name (type): description\"\n",
    "            parts = line.split(':', 1)\n",
    "            param_part = parts[0].strip()\n",
    "            desc = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            # Extract param_name from \"param_name (type)\"\n",
    "            if '(' in param_part and ')' in param_part:\n",
    "                param_name = param_part[:param_part.index('(')].strip()\n",
    "            else:\n",
    "                param_name = param_part\n",
    "            if param_name:\n",
    "                param_descriptions[param_name] = desc\n",
    "\n",
    "    # Map Python types to JSON schema types\n",
    "    type_mapping = {\n",
    "        'str': 'string',\n",
    "        'int': 'integer',\n",
    "        'float': 'number',\n",
    "        'bool': 'boolean',\n",
    "        'dict': 'object',\n",
    "        'list': 'array'\n",
    "    }\n",
    "\n",
    "    # Build schema\n",
    "    schema = {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": description,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": [],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Add parameters to schema\n",
    "    for param_name, param in signature.parameters.items():\n",
    "        # Get type from type hints, default to str if not specified\n",
    "        param_type = type_hints.get(param_name, str).__name__\n",
    "        schema_type = type_mapping.get(param_type, 'string')  # Default to string\n",
    "        \n",
    "        schema[\"function\"][\"parameters\"][\"properties\"][param_name] = {\n",
    "            \"type\": schema_type,\n",
    "            \"description\": param_descriptions.get(param_name, f\"{param_name} parameter\")\n",
    "        }\n",
    "        # Mark as required if no default value\n",
    "        if param.default == inspect.Parameter.empty and param.kind != inspect.Parameter.VAR_KEYWORD:\n",
    "            schema[\"function\"][\"parameters\"][\"required\"].append(param_name)\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa6e45",
   "metadata": {},
   "source": [
    "#### Weather function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get current temperature for a given location.\n",
    "\n",
    "    Args:\n",
    "        location (str): City and country e.g. Bogotá, Colombia\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the current temperature and other weather details.\n",
    "    \"\"\"\n",
    "    return {\"location\": location, \"temperature\": 25, \"unit\": \"Celsius\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f28a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_json_schema(func=get_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93146159",
   "metadata": {},
   "source": [
    "#### Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3afe1",
   "metadata": {},
   "source": [
    "| 4. Streaming |\n",
    "|--|\n",
    "\n",
    "\n",
    "Streaming responses lets you start printing or processing the beginning of the model's output while it continues generating the full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times fast.\",\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4138e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in stream:\n",
    "    print(chunk)\n",
    "    print(chunk.choices[0].delta)\n",
    "    print(\"****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f180a",
   "metadata": {},
   "source": [
    "| 5. Assistants |\n",
    "|--|\n",
    "\n",
    "Build assistants that can call models and use tools to perform tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75ad6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_assistant = client.beta.assistants.create(\n",
    "    instructions=\"You are a personal math tutor. When asked a question, write and run Python code to answer the question.\",\n",
    "    name=\"Math Tutor\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726d96d",
   "metadata": {},
   "source": [
    "function pass, python function (simple) as tool. Re-use after create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40bd7378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asst_ByArr4GEQk8a6p76UoZ5UvXj\n"
     ]
    }
   ],
   "source": [
    "print(my_assistant.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866a1b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('You are a comedian who tells short, family-friendly jokes.',\n",
       " '# **Context**  \\nYou are **Ava**, a professional and friendly AI chat assistant representing **Amplit')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_assistants = client.beta.assistants.list(\n",
    "    order=\"desc\",\n",
    "    limit=\"20\",\n",
    ")\n",
    "\n",
    "my_assistants.data[0].instructions[:100], my_assistants.data[1].instructions[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02addbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_assistants.data[1].instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde623fa",
   "metadata": {},
   "source": [
    "| Math Tool |\n",
    "|--|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a1332e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tell_joke(topic: str) -> float:\n",
    "    \"\"\"Tells a joke about topic.\"\"\"\n",
    "    return f\"Why did the chicken say boo about {topic}\"\n",
    "\n",
    "# Define the tool schema for the addition function\n",
    "tools = {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"tell_joke\",\n",
    "    \"description\": \"Generates a short, family-friendly joke about a specified topic.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"topic\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The topic or subject of the joke (e.g., 'cats', 'space', 'computers').\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"topic\"],\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eb44fc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Assistant(id='asst_ByArr4GEQk8a6p76UoZ5UvXj', created_at=1748349069, description=None, instructions='You are a comedian who tells short, family-friendly jokes.', metadata={}, model='gpt-4o-mini', name='Joke Teller', object='assistant', tools=[FunctionTool(function=FunctionDefinition(name='tell_joke', description='Generates a short, family-friendly joke about a specified topic.', parameters={'type': 'object', 'properties': {'topic': {'type': 'string', 'description': \"The topic or subject of the joke (e.g., 'cats', 'space', 'computers').\"}}, 'required': ['topic'], 'additionalProperties': False}, strict=False), type='function')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.beta.assistants.update(assistant_id=my_assistant.id, \n",
    "    instructions=\"You are a comedian who tells short, family-friendly jokes.\",\n",
    "    name=\"Joke Teller\",\n",
    "    model=MODEL_NAME,\n",
    "    tools=[tools]  # Pass the addition tool\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b70fb",
   "metadata": {},
   "source": [
    "Assistant with Tools usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "05264e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created thread with ID: thread_sfVieYY4xRXeccrYdSVNWGSi\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a thread\n",
    "thread = client.beta.threads.create()\n",
    "print(f\"Created thread with ID: {thread.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6dfe5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add a message to the thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Joke about cats\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "918b34e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=my_assistant.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    if run_status.status == \"completed\":\n",
    "        break\n",
    "\n",
    "# Step 5: Retrieve and print the assistant's response\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "for msg in messages.data:\n",
    "    print(f\"{msg.role.capitalize()}: {msg.content[0].text.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc12d6",
   "metadata": {},
   "source": [
    "| Threads |\n",
    "|--|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062ce89",
   "metadata": {},
   "source": [
    "Create a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5899a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()\n",
    "thread_id = thread.id\n",
    "print(f\"Thread ID: {thread_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba5698",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread_id,\n",
    "    role=\"user\",\n",
    "    content=\"Explain factorial?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread_id,\n",
    "    assistant_id=my_assistant.id\n",
    ")\n",
    "\n",
    "# Wait for the run to complete\n",
    "import time\n",
    "while True:\n",
    "    run_status = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)\n",
    "    if run_status.status == \"completed\":\n",
    "        break\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "for message in messages.data:\n",
    "    if message.role == \"assistant\":\n",
    "        print(f\"Assistant: {message.content[0].text.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca8432",
   "metadata": {},
   "source": [
    "| 6. Response API |\n",
    "|--|\n",
    "\n",
    "Allow the model access to external systems and data using function calling.\n",
    "\n",
    "We get the option to outsource that to OpenAI entirely: you can add a new \"store\": true property and then in subsequent messages include a \"previous_response_id: response_id key to continue that conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af61461",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739998c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=\"What was a positive news story from today?\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08481dbb",
   "metadata": {},
   "source": [
    "Response API with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4fc626a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmcp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserver_label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshopify\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserver_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://pitchskin.com/api/mcp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdd the Blemish Toner Pads to my cart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/learning-bHz_lsOL/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/learning-bHz_lsOL/lib/python3.10/site-packages/openai/resources/responses/responses.py:684\u001b[0m, in \u001b[0;36mResponses.create\u001b[0;34m(self, input, model, background, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    683\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response \u001b[38;5;241m|\u001b[39m Stream[ResponseStreamEvent]:\n\u001b[0;32m--> 684\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/responses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstructions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_output_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprevious_response_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/learning-bHz_lsOL/lib/python3.10/site-packages/openai/_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1227\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1236\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1237\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/learning-bHz_lsOL/lib/python3.10/site-packages/openai/_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "  model=MODEL_NAME,\n",
    "  tools=[{\n",
    "    \"type\": \"mcp\",\n",
    "    \"server_label\": \"shopify\",\n",
    "    \"server_url\": \"https://pitchskin.com/api/mcp\",\n",
    "  }],\n",
    "  input=\"Add the Blemish Toner Pads to my cart\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b720628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320db314",
   "metadata": {},
   "source": [
    "| 7. Response Chaining |\n",
    "|--|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0b80677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response (story idea): When a disillusioned historian discovers a way to travel back to pivotal moments in history, she must choose between altering a tragic event for the better or preserving the timeline, knowing that her decision could erase her own existence.\n",
      "\n",
      "Second response (story outline):\n",
      " **Title:** Echoes of Time\n",
      "\n",
      "**Outline:**\n",
      "\n",
      "**I. Introduction**\n",
      "- **A. Protagonist:** Introduce Rachel, a disillusioned historian specializing in tragic events. She feels her work is futile as history tends to repeat itself.\n",
      "- **B. Discovery:** Rachel stumbles upon an ancient artifact in a dusty library that allows time travel to pivotal moments in history.\n",
      "- **C. Motivation:** Driven by personal loss due to a tragic event (e.g., a war or natural disaster), Rachel is tempted to change history for the better.\n",
      "\n",
      "**II. First Journey: A Test Run**\n",
      "- **A. Excursion:** Rachel travels to a minor historical event to test the artifact, experiencing the thrill of being a direct observer.\n",
      "- **B. Consequences:** She unintentionally alters a small detail, leading to unexpected yet minor changes in her present life.\n",
      "- **C. Realization:** Rachel grapples with the implications of change and begins to contemplate altering larger, more significant events.\n",
      "\n",
      "**III. The Pivotal Moment**\n",
      "- **A. Decision Time:** Rachel researches an event of historical tragedy (e.g., a famous battle, assassination, etc.) that has haunted her since childhood.\n",
      "- **B. Ethical Dilemma:** Rachel encounters supporters and skeptics of historical alteration, leading to internal conflict about her potential impact.\n",
      "- **C. Motivation Deepens:** In a flashback, Rachel recalls personal stakes—how this event affected her family or community.\n",
      "\n",
      "**IV. The Journey to Change History**\n",
      "- **A. Time Travel:** Rachel successfully travels to the chosen moment, filled with resolve to change the course of history.\n",
      "- **B. Plight:** She discovers complexities she hadn’t anticipated, such as political struggles, personal stories, and the lives of those involved.\n",
      "- **C. Moment of Truth:** Rachel faces a crucial choice at the moment when tragedy strikes, weighing immediate emotional relief against the long-term effects on historical continuity.\n",
      "\n",
      "**V. The Aftermath**\n",
      "- **A. Consequence of the Act:** Rachel makes her choice and alters the tragic event, however, she is thrust back into her present.\n",
      "- **B. Changes Unfold:** The world around her begins to change dramatically, both positively and negatively. She sees improvements but also unintended results and suffering that arise from her intervention.\n",
      "- **C. Self-Discovery:** Rachel begins to understand that pain is a part of human experience and that every moment shapes history in uniquely profound ways.\n",
      "\n",
      "**VI. Climax: The Return to Original Timeline**\n",
      "- **A. Struggle with Existence:** As the timeline shifts, \n"
     ]
    }
   ],
   "source": [
    "first_prompt = \"Generate a one-sentence idea for a short story about a time traveler.\"\n",
    "response1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": first_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the output from the first call\n",
    "story_idea = response1.choices[0].message.content\n",
    "print(\"First response (story idea):\", story_idea)\n",
    "\n",
    "# Second API call: Use the first response to expand into a brief outline\n",
    "second_prompt = f\"Create a brief outline for a short story based on this idea: {story_idea}\"\n",
    "response2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": second_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and print the second response\n",
    "story_outline = response2.choices[0].message.content\n",
    "print(\"\\nSecond response (story outline):\\n\", story_outline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-bHz_lsOL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
